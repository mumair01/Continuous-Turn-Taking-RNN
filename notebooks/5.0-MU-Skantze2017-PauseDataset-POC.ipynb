{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About \n",
    "\n",
    "This notebook is intended to be a POC for the PauseDataset required for Experiment 4.2 in the Skantze 2017 paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Download libraries for environment. \n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "# Env. vars to check if the notebook is running on colab, kaggle etc. \n",
    "IS_COLAB = \"google.colab\" in sys.modules \n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules \n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Install the packages \n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "    # Mount the drive \n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy \n",
    "\n",
    "import random \n",
    "import shutil \n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Others \n",
    "import glob \n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Set environment global vars. \n",
    "\n",
    "# Shared env. vars. \n",
    "GLOBAL_SEED = 42 \n",
    "IS_CUDA_ENV = torch.cuda.is_available()\n",
    "GLOBAL_DEVICE = torch.device('cuda') if IS_CUDA_ENV else torch.device('cpu')\n",
    "SET_SEED = True # If true, sets the global seeds for this notebook. \n",
    "LIMITED_RESOURCES = not IS_CUDA_ENV\n",
    "\n",
    "if LIMITED_RESOURCES:\n",
    "    SMALL_DATASET_SIZE = 10\n",
    "\n",
    "if IS_COLAB:\n",
    "    LIMITED_RESOURCES = False \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring env. \n",
    "if SET_SEED:\n",
    "    # to make this notebook's output stable across runs\n",
    "    np.random.seed(GLOBAL_SEED) \n",
    "    torch.manual_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Paths\n",
    "NOTEBOOK_NAME = \"skantze2017_pause_dataset_poc\"\n",
    "PROJECT_ROOT_DIR = \"/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous\" \n",
    "# --- Input data dirs. \n",
    "DATASET_NAME = \"maptask\"\n",
    "DATASET_TYPE = \"csv\"\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"raw\", \"maptask\")\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", NOTEBOOK_NAME)\n",
    "\n",
    "# --- Result dirs. \n",
    "# NOTE: The model dir will have to change depending on where the models are stored. \n",
    "REPORTS_DIR = os.path.join(PROJECT_ROOT_DIR,\"reports\",NOTEBOOK_NAME)\n",
    "\n",
    "# Paths to the specific feature sets \n",
    "FULL_PROCESSED_FEATURE_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", \"maptask\",\"full\")\n",
    "PROSODY_PROCESSED_FEATURE_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", \"maptask\",\"prosody\")\n",
    "FULL_PROCESSED_FEATURE_DIR\n",
    "\n",
    "os.makedirs(REPORTS_DIR,exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR,exist_ok=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Code \n",
    "\n",
    "**NOTE** In this section, we are copying some required code from notebooks 1.0-* to 3.0-*.\n",
    "\n",
    "Ay bugs in this code **should be fixed in the appropriate notebooks**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapTask Dataset Generation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"raw\", \"maptask\")\n",
    "MAPTASK_DIR = os.path.join(RAW_DATA_DIR,\"maptaskv2-1\")\n",
    "# Paths within the maptask corpus \n",
    "STEREO_AUDIO_PATH = os.path.join(MAPTASK_DIR,\"Data/signals/dialogues\")\n",
    "MONO_AUDIO_PATH = os.path.join(MAPTASK_DIR,\"Data/signals/mono_signals\")\n",
    "# NOTE: The timed units are also used for Voice Activity annotations. \n",
    "TIMED_UNIT_PATHS = os.path.join(MAPTASK_DIR,\"Data/timed-units\") \n",
    "POS_PATH = os.path.join(MAPTASK_DIR,\"Data/pos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maptask_participant(csv_path):\n",
    "    filename, ext = os.path.splitext(os.path.basename(csv_path))\n",
    "    filename_split = filename.split(\".\")\n",
    "    participant = filename_split[1]\n",
    "    return participant\n",
    "\n",
    "def get_maptask_dialogue(csv_path):\n",
    "    filename, ext = os.path.splitext(os.path.basename(csv_path))\n",
    "    filename_split = filename.split(\".\")\n",
    "    dialogue = filename_split[0]\n",
    "    return dialogue\n",
    "\n",
    "def read_data(dir_path,dialogue_name, participant,ext):\n",
    "    \"\"\"\n",
    "    Assumption is that the basename . is the dialogue name. \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    data_paths = [p for p in os.listdir(dir_path)]\n",
    "    data_paths = [os.path.join(dir_path,p) for p in data_paths if os.path.splitext(p)[1][1:] == ext]\n",
    "    for path in data_paths:\n",
    "       if get_maptask_dialogue(path) == dialogue_name and \\\n",
    "                get_maptask_participant(path) == participant:\n",
    "            results.append(path)\n",
    "    return results \n",
    "\n",
    "def get_mono_audio(dialogue_name, participant):\n",
    "    return read_data(MONO_AUDIO_PATH,dialogue_name, participant,\"wav\")[0]\n",
    "\n",
    "def get_stereo_audio(dialogue_name):\n",
    "    return read_data(STEREO_AUDIO_PATH,dialogue_name,\"mix\",\"wav\")[0]\n",
    "\n",
    "def get_timed_unit(dialogue_name, participant):\n",
    "    return read_data(TIMED_UNIT_PATHS,dialogue_name, participant,\"xml\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dialogue_features(dialogue_names, features_dir):\n",
    "    \"\"\"\n",
    "    Collect the dialogue f and g feature files.\n",
    "    Assumes that features_dir contains both the f and g feature files. \n",
    "    \"\"\"\n",
    "    collected = {}\n",
    "    for dialogue in dialogue_names:\n",
    "        collected[dialogue] = {\n",
    "            \"f\" : read_data(features_dir,dialogue,\"f\",\"csv\")[0], \n",
    "            \"g\" : read_data(features_dir,dialogue,\"g\",\"csv\")[0]}\n",
    "    return collected \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_dialogues(dataset_paths, test_size=0.25, val_size=0.2, \n",
    "        seed=GLOBAL_SEED):\n",
    "    dataset_paths = deepcopy(dataset_paths)\n",
    "    dialogue_names = sorted(list(set([get_maptask_dialogue(p) for p in dataset_paths])))\n",
    "    train_dialogues, test_dialogues = train_test_split(dialogue_names, \n",
    "        test_size=test_size,random_state=seed)\n",
    "    train_dialogues, val_dialogues = train_test_split(train_dialogues, \n",
    "        test_size=val_size,random_state=seed)\n",
    "    return train_dialogues, val_dialogues, test_dialogues \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapTask Dataclass and Method Definitions \n",
    "\n",
    "These are required to be in the same notebook to be loaded. \n",
    "\n",
    "\n",
    "NOTE: **DO NOT** modify these here, refer to 2.0-MU-Skantze-MapTask-Dataset-POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:Seed worker can be used to ensure reproducibility in DataLoader \n",
    "# across runs. \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed =GLOBAL_SEED\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def generate_dataloader(dataset, batch_size=32, shuffle=True, num_workers=0, \n",
    "        drop_last=True, pin_memory=True):\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        num_workers=num_workers, \n",
    "        drop_last=drop_last, # We always want to remove the last incomplete batch. \n",
    "        pin_memory=pin_memory, \n",
    "        worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.2: MapTask Pause Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skantze2017PausesDataset(Dataset):\n",
    "\n",
    "    HOLD_LABEL = 0 \n",
    "    SHIFT_LABEL = 1 \n",
    "\n",
    "    def __init__(self, feature_paths_map , sequence_length_ms, min_pause_length_ms, \n",
    "                max_future_silence_window_ms, s0_participant, frame_step_size_ms, \n",
    "                save_dir = None):\n",
    "        # Params . \n",
    "        self.feature_paths_map = feature_paths_map \n",
    "        self.sequence_length_ms = sequence_length_ms \n",
    "        self.min_pause_length_ms = min_pause_length_ms\n",
    "        self.max_future_silence_window_ms = max_future_silence_window_ms\n",
    "        self.s0_participant = s0_participant\n",
    "        self.frame_step_size_ms = frame_step_size_ms  \n",
    "        self.save_dir = save_dir # If the save dir is provided, saves the dataset as it is built. \n",
    "        # Calculated \n",
    "        self.num_context_frames = int(sequence_length_ms / frame_step_size_ms)\n",
    "        self.num_pause_frames = int(min_pause_length_ms / frame_step_size_ms)\n",
    "        self.future_window_frames = int(max_future_silence_window_ms / frame_step_size_ms)\n",
    "        # Data Storage vars. \n",
    "        self.xs = [] \n",
    "        self.ys = [] \n",
    "        self.num_silences = 0 \n",
    "        self.num_holds = 0 \n",
    "        self.num_shifts = 0 \n",
    "        self.num_pauses = 0 \n",
    "        # Prepare the data \n",
    "        for dialogue in list(self.feature_paths_map.keys()):\n",
    "            self.__prepare_items(dialogue, self.__prepare_pauses_df(dialogue))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx > self.__len__():\n",
    "            raise Exception \n",
    "        # NOTE: xs has target speaker features concatenated with non-target speaker features. \n",
    "        # ys is the previous speaker, hold / shift label, and the next speaker. \n",
    "        return self.xs[idx], self.ys[idx]\n",
    "    \n",
    "    def get_pause_statistics(self):\n",
    "        return {\n",
    "            \"min_pause_length_ms \" : self.min_pause_length_ms , \n",
    "            \"sequence_length_ms\" : self.sequence_length_ms, \n",
    "            \"max_future_silence_window_ms\" : self.max_future_silence_window_ms, \n",
    "            \"num_silences\" : self.num_silences, \n",
    "            \"num_holds\" : self.num_holds, \n",
    "            \"num_shifts\" : self.num_shifts,\n",
    "            \"num_pauses\" : self.num_pauses,\n",
    "            \"s0_participant\" : self.s0_participant\n",
    "        }\n",
    "    \n",
    "    def __prepare_items(self,dialogue,pauses_df):\n",
    "        s0_feature_df, s1_feature_df = self.__load_dataframes(dialogue)\n",
    "        # Collect the data for both models\n",
    "        s0_s1_df = pd.concat([s0_feature_df.loc[:,s0_feature_df.columns != 'frameTime'],s1_feature_df.loc[:,s1_feature_df.columns != 'frameTime']],axis=1)   \n",
    "        s1_s0_df = pd.concat([s1_feature_df.loc[:,s1_feature_df.columns != 'frameTime'],s0_feature_df.loc[:,s0_feature_df.columns != 'frameTime']],axis=1) \n",
    "        for pause_data in pauses_df.itertuples():\n",
    "            _,_, previous_speaker, idx_after_silence_frames, _, hold_shift_label, next_speaker = pause_data \n",
    "            idx_after_silence_frames = int(idx_after_silence_frames)\n",
    "            # Collect features for each speaker equal to sequence length / num context frames. \n",
    "            if idx_after_silence_frames- self.num_context_frames >= 0:\n",
    "                x_s0 = np.asarray(s0_s1_df.iloc[idx_after_silence_frames-self.num_context_frames:idx_after_silence_frames])\n",
    "                x_s1 = np.asarray(s1_s0_df.iloc[idx_after_silence_frames-self.num_context_frames:idx_after_silence_frames])\n",
    "            else:\n",
    "                num_pad = self.num_context_frames - idx_after_silence_frames -1 \n",
    "                x_s0 = np.pad(np.asarray(s0_s1_df.iloc[0:idx_after_silence_frames+1]),[(num_pad,0),(0,0)],'constant')\n",
    "                x_s1 = np.pad(np.asarray(s1_s0_df.iloc[0:idx_after_silence_frames+1]),[(num_pad,0),(0,0)],'constant')\n",
    "            self.xs.append((x_s0,x_s1)) \n",
    "            self.ys.append((int(previous_speaker), int(hold_shift_label), int(next_speaker)))\n",
    "\n",
    "    def __prepare_pauses_df(self, dialogue):\n",
    "        s0_feature_df, s1_feature_df = self.__load_dataframes(dialogue)\n",
    "        # Obtain frame indices where both speakers are speaking. \n",
    "        s0_va_idxs =  np.where(s0_feature_df['voiceActivity'] == 1)[0]\n",
    "        s1_va_idxs =  np.where(s1_feature_df['voiceActivity'] == 1)[0]\n",
    "        va_idxs = np.union1d(s0_va_idxs,s1_va_idxs)\n",
    "        # Obtain index of last speaking frame before silences \n",
    "        speak_before_silence_frames_idx = \\\n",
    "            va_idxs[np.where(np.diff(va_idxs) > self.num_pause_frames)]\n",
    "        self.num_silences += len(speak_before_silence_frames_idx)\n",
    "        # Remove scenarios where both speakers were speaking last i.e., only \n",
    "        # one speaking could have been speaking before te pause\n",
    "        speak_before_silence_frames_idx = [idx for idx in speak_before_silence_frames_idx \\\n",
    "            if not (idx in s0_va_idxs and idx in s1_va_idxs) and (idx in s0_va_idxs or idx in s1_va_idxs)]\n",
    "        # Next, we want to find all the instances where one (and only one) \n",
    "        # speaker continues within the next future_window_ms seconds. \n",
    "        pauses_df = pd.DataFrame(columns=['pauseStartFrameTime', 'previousSpeaker',\n",
    "            'pauseEndFrameIndex', 'nextSpeechFrameIndex', 'holdShiftLabel', 'nextSpeaker'])\n",
    "        for i,idx in enumerate(speak_before_silence_frames_idx):\n",
    "            # Obtain index of the frame after the specified pause length. \n",
    "            idx_after_silence_frames = idx + self.num_pause_frames + 1 \n",
    "            # Get the voice activity in the specified future window\n",
    "            s0_window_va = np.asarray((s0_feature_df['voiceActivity'])[\n",
    "                idx_after_silence_frames:idx_after_silence_frames+self.future_window_frames] == 1)\n",
    "            s1_window_va = np.asarray((s1_feature_df['voiceActivity'])[\n",
    "                idx_after_silence_frames:idx_after_silence_frames+self.future_window_frames] == 1)\n",
    "            # Determine the last speaker before silence \n",
    "            last_participant = 0 if s0_feature_df['voiceActivity'].iloc[idx] else 1 \n",
    "            # NOTE: Both speakers might start speaking in the future window but we want \n",
    "            # to make sure that only one of the speakers starts i.e., no overlap. \n",
    "            # NOTE: 0 = hold, 1 = shift \n",
    "            # Condition 1: Speaker 0 is next. \n",
    "            if s0_window_va.any() and not s1_window_va.any():\n",
    "                next_va_idx = np.argmax(s0_window_va) + idx_after_silence_frames  \n",
    "                hold_shift_label = self.HOLD_LABEL if last_participant == 0 else self.SHIFT_LABEL\n",
    "                pauses_df.loc[i] = (\n",
    "                    s0_feature_df.iloc[idx]['frameTime'], last_participant,\n",
    "                     idx_after_silence_frames, next_va_idx, hold_shift_label,0)             \n",
    "            # Condition 2: Speaker 1 is next. \n",
    "            elif s1_window_va.any() and not s0_window_va.any():\n",
    "                next_va_idx = np.argmax(s1_window_va) + idx_after_silence_frames \n",
    "                hold_shift_label = self.HOLD_LABEL if last_participant == 1 else self.SHIFT_LABEL\n",
    "                pauses_df.loc[i] = (\n",
    "                    s1_feature_df.iloc[idx]['frameTime'], last_participant,\n",
    "                    idx_after_silence_frames, next_va_idx, hold_shift_label,1) \n",
    "        # Update the shift / hold values \n",
    "        self.num_holds += (pauses_df['holdShiftLabel'] == self.HOLD_LABEL).sum()\n",
    "        self.num_shifts += (pauses_df['holdShiftLabel'] == self.SHIFT_LABEL).sum()\n",
    "        self.num_pauses = self.num_holds + self.num_shifts\n",
    "        # Save the pauses df if save dir provided \n",
    "        if self.save_dir != None:\n",
    "            pauses_df.to_csv(\"{}/{}_pauses_df.csv\".format(self.save_dir, dialogue))\n",
    "        return pauses_df \n",
    "\n",
    "    def __load_dataframes(self, dialogue):\n",
    "        if self.s0_participant == \"f\":\n",
    "            s0_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"f\"], index_col=0,delimiter=\",\") \n",
    "            s1_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"g\"],index_col=0,delimiter=\",\")\n",
    "        else:\n",
    "            s0_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"g\"],index_col=0,delimiter=\",\") \n",
    "            s1_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"f\"],index_col=0,delimiter=\",\")\n",
    "        # Trim the dataframes to the same length \n",
    "        min_num_frames = np.min([len(s0_feature_df.index),len(s1_feature_df.index)])\n",
    "        s0_feature_df = s0_feature_df[:min_num_frames]\n",
    "        s1_feature_df = s1_feature_df[:min_num_frames]\n",
    "        assert len(s0_feature_df) == len(s1_feature_df)\n",
    "        return s0_feature_df, s1_feature_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_STEP_SIZE_MS = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the processed data. \n",
    "dataset_csv_paths =  glob.glob(\"{}/*.csv\".format(FULL_PROCESSED_FEATURE_DIR))\n",
    "\n",
    "if LIMITED_RESOURCES:\n",
    "    dataset_csv_paths = dataset_csv_paths[:SMALL_DATASET_SIZE]\n",
    "\n",
    "len(dataset_csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dialogues, val_dialogues, test_dialogues = get_train_val_test_dialogues(dataset_csv_paths)\n",
    "len(train_dialogues), len(val_dialogues), len(test_dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_paths_map =  collect_dialogue_features(\n",
    "    train_dialogues,FULL_PROCESSED_FEATURE_DIR)\n",
    "len(feature_paths_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_pauses_dataset = Skantze2017PausesDataset(\n",
    "    feature_paths_map=feature_paths_map,\n",
    "    sequence_length_ms= 60_000, \n",
    "    min_pause_length_ms=500,  \n",
    "    max_future_silence_window_ms=1000, \n",
    "    s0_participant=\"f\",\n",
    "    frame_step_size_ms=FRAME_STEP_SIZE_MS,\n",
    "    save_dir = PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_pause_length_ms ': 500,\n",
       " 'sequence_length_ms': 60000,\n",
       " 'max_future_silence_window_ms': 1000,\n",
       " 'num_silences': 348,\n",
       " 'num_holds': 129,\n",
       " 'num_shifts': 119,\n",
       " 'num_pauses': 248,\n",
       " 's0_participant': 'f'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pauses_dataset.get_pause_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = generate_dataloader(test_pauses_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1200, 130]) torch.Size([1, 1200, 130])\n",
      "[tensor([0]), tensor([1]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in dataloader:\n",
    "    x_s0, x_s1 = x_batch \n",
    "    print(x_s0.shape, x_s1.shape)\n",
    "    print(y_batch)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('trp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2304c455ac551f7a45279e05896e7a6c75d8c847965e97ef85e853390a9b358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
