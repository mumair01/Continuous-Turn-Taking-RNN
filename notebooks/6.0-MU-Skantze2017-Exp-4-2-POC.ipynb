{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About \n",
    "\n",
    "This notebook is intended to be a POC for Experiment 4.2 in the Skantze 2017 paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Download libraries for environment. \n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "# Env. vars to check if the notebook is running on colab, kaggle etc. \n",
    "IS_COLAB = \"google.colab\" in sys.modules \n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules \n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Install the packages \n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "    # Mount the drive \n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy \n",
    "\n",
    "import random \n",
    "import shutil \n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Others \n",
    "import glob \n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Set environment global vars. \n",
    "\n",
    "# Shared env. vars. \n",
    "GLOBAL_SEED = 42 \n",
    "IS_CUDA_ENV = torch.cuda.is_available()\n",
    "GLOBAL_DEVICE = torch.device('cuda') if IS_CUDA_ENV else torch.device('cpu')\n",
    "SET_SEED = True # If true, sets the global seeds for this notebook. \n",
    "LIMITED_RESOURCES = not IS_CUDA_ENV\n",
    "\n",
    "if LIMITED_RESOURCES:\n",
    "    SMALL_DATASET_SIZE = 20\n",
    "\n",
    "if IS_COLAB:\n",
    "    LIMITED_RESOURCES = False \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring env. \n",
    "if SET_SEED:\n",
    "    # to make this notebook's output stable across runs\n",
    "    np.random.seed(GLOBAL_SEED) \n",
    "    torch.manual_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Paths\n",
    "NOTEBOOK_NAME = \"skantze2017_exp_4_2_poc\"\n",
    "PROJECT_ROOT_DIR = \"/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous\" \n",
    "# --- Input data dirs. \n",
    "\n",
    "# --- Result dirs. \n",
    "# NOTE: The model dir will have to change depending on where the models are stored. \n",
    "PROCESSED_DATA_DIR =  os.path.join(PROJECT_ROOT_DIR,\"data\",\"processed\",NOTEBOOK_NAME)\n",
    "REPORTS_DIR = os.path.join(PROJECT_ROOT_DIR,\"reports\",NOTEBOOK_NAME)\n",
    "SAVE_MODELS_DIR = os.path.join(PROJECT_ROOT_DIR,\"models\",NOTEBOOK_NAME)\n",
    "\n",
    "# Paths to the specific feature sets \n",
    "FULL_PROCESSED_FEATURE_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", \"maptask\",\"full\")\n",
    "PROSODY_PROCESSED_FEATURE_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"processed\", \"maptask\",\"prosody\")\n",
    "FULL_PROCESSED_FEATURE_DIR\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR,exist_ok=True)\n",
    "os.makedirs(REPORTS_DIR,exist_ok=True)\n",
    "os.makedirs(SAVE_MODELS_DIR,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT_DIR,\"data\", \"raw\", \"maptask\")\n",
    "MAPTASK_DIR = os.path.join(RAW_DATA_DIR,\"maptaskv2-1\")\n",
    "# Paths within the maptask corpus \n",
    "STEREO_AUDIO_PATH = os.path.join(MAPTASK_DIR,\"Data/signals/dialogues\")\n",
    "MONO_AUDIO_PATH = os.path.join(MAPTASK_DIR,\"Data/signals/mono_signals\")\n",
    "# NOTE: The timed units are also used for Voice Activity annotations. \n",
    "TIMED_UNIT_PATHS = os.path.join(MAPTASK_DIR,\"Data/timed-units\") \n",
    "POS_PATH = os.path.join(MAPTASK_DIR,\"Data/pos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(path, fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(path, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Code \n",
    "\n",
    "**NOTE** In this section, we are copying some required code from notebooks 1.0-* to 3.0-*.\n",
    "\n",
    "Ay bugs in this code **should be fixed in the appropriate notebooks**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapTask Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maptask_participant(csv_path):\n",
    "    filename, ext = os.path.splitext(os.path.basename(csv_path))\n",
    "    filename_split = filename.split(\".\")\n",
    "    participant = filename_split[1]\n",
    "    return participant\n",
    "\n",
    "def get_maptask_dialogue(csv_path):\n",
    "    filename, ext = os.path.splitext(os.path.basename(csv_path))\n",
    "    filename_split = filename.split(\".\")\n",
    "    dialogue = filename_split[0]\n",
    "    return dialogue\n",
    "\n",
    "def read_data(dir_path,dialogue_name, participant,ext):\n",
    "    \"\"\"\n",
    "    Assumption is that the basename . is the dialogue name. \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    data_paths = [p for p in os.listdir(dir_path)]\n",
    "    data_paths = [os.path.join(dir_path,p) for p in data_paths if os.path.splitext(p)[1][1:] == ext]\n",
    "    for path in data_paths:\n",
    "       if get_maptask_dialogue(path) == dialogue_name and \\\n",
    "                get_maptask_participant(path) == participant:\n",
    "            results.append(path)\n",
    "    return results \n",
    "\n",
    "def get_mono_audio(dialogue_name, participant):\n",
    "    return read_data(MONO_AUDIO_PATH,dialogue_name, participant,\"wav\")[0]\n",
    "\n",
    "def get_stereo_audio(dialogue_name):\n",
    "    return read_data(STEREO_AUDIO_PATH,dialogue_name,\"mix\",\"wav\")[0]\n",
    "\n",
    "def get_timed_unit(dialogue_name, participant):\n",
    "    return read_data(TIMED_UNIT_PATHS,dialogue_name, participant,\"xml\")[0]\n",
    "\n",
    "def collect_dialogue_features(dialogue_names, features_dir):\n",
    "    \"\"\"\n",
    "    Collect the dialogue f and g feature files.\n",
    "    Assumes that features_dir contains both the f and g feature files. \n",
    "    \"\"\"\n",
    "    collected = {}\n",
    "    for dialogue in dialogue_names:\n",
    "        collected[dialogue] = {\n",
    "            \"f\" : read_data(features_dir,dialogue,\"f\",\"csv\")[0], \n",
    "            \"g\" : read_data(features_dir,dialogue,\"g\",\"csv\")[0]}\n",
    "    return collected \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_voice_activity_labels(feature_df, N):\n",
    "    # TODO: FIx the delimiter \n",
    "    feature_df = feature_df[[\"frameTime\",\"voiceActivity\"]]\n",
    "    assert not feature_df.isnull().values.any()\n",
    "    frame_times_ms = np.asarray(feature_df[\"frameTime\"])\n",
    "    voice_activity_annotations = np.asarray(feature_df[\"voiceActivity\"])\n",
    "    assert frame_times_ms.shape[0] == voice_activity_annotations.shape[0]\n",
    "    labels = np.zeros((frame_times_ms.shape[0],N)) # target label shape: Num Frames x N\n",
    "    for i in range(len(frame_times_ms)):\n",
    "        # Pad the last labels with 0 if the conversation has ended \n",
    "        if i + N > len(frame_times_ms):\n",
    "            concat = np.concatenate(\n",
    "                [voice_activity_annotations[i:],\n",
    "                 np.zeros(N - (len(frame_times_ms)-i))])\n",
    "            labels[i] = concat\n",
    "        else:\n",
    "            labels[i] = voice_activity_annotations[i:i+N]\n",
    "    labels_df = pd.DataFrame(labels) \n",
    "    labels_df.insert(0,\"frameTime\",frame_times_ms)\n",
    "    assert not labels_df.isnull().values.any()\n",
    "    return labels_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapTask Dataset Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Skantze2017VAPredictionMapTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Maptask dataset for voice activity annotation sequence prediction.  \n",
    "    NOTE: Needs a large amount of memory to load this. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_paths_map, sequence_length_ms, \n",
    "            prediction_length_ms, target_participant, frame_step_size_ms):\n",
    "        # Vars. \n",
    "        self.feature_paths_map = feature_paths_map \n",
    "        self.sequence_length_ms = sequence_length_ms \n",
    "        self.prediction_length_ms = prediction_length_ms \n",
    "        self.target_participant = target_participant \n",
    "        self.frame_step_size_ms = frame_step_size_ms \n",
    "        # Calculated \n",
    "        self.num_context_frames = int(sequence_length_ms / frame_step_size_ms)\n",
    "        self.num_target_frames = int(prediction_length_ms / frame_step_size_ms)\n",
    "        # Storage \n",
    "        self.xs = [] \n",
    "        self.ys = [] \n",
    "        for dialogue in list(self.feature_paths_map.keys()):\n",
    "            self.__load_data(dialogue)\n",
    "        assert len(self.xs) == len(self.ys)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx > self.__len__():\n",
    "            raise Exception \n",
    "        return self.xs[idx], self.ys[idx]\n",
    "\n",
    "    def __load_data(self, dialogue):\n",
    "        s0_feature_df, s1_feature_df = self.__load_dataframes(dialogue)\n",
    "        # Extract the voice activity labels for s0 as the target labels\n",
    "        s0_target_labels_df = extract_voice_activity_labels(\n",
    "            s0_feature_df,self.num_target_frames)\n",
    "        # Make sure none of the dfs have any nan values \n",
    "        assert not s0_feature_df.isnull().values.any() and \\\n",
    "            not s1_feature_df.isnull().values.any() and \\\n",
    "            not s0_target_labels_df.isnull().values.any()\n",
    "        # Trim the dataframes to the same length \n",
    "        min_num_frames = np.min([len(s0_feature_df.index),len(s1_feature_df.index)])\n",
    "        s0_feature_df = s0_feature_df[:min_num_frames]\n",
    "        s1_feature_df = s1_feature_df[:min_num_frames]\n",
    "        s0_target_labels_df = s0_target_labels_df[:min_num_frames]\n",
    "        # Make sure they all have common frametimes\n",
    "        assert s0_feature_df['frameTime'].equals(s1_feature_df['frameTime'])\n",
    "        assert s0_feature_df['frameTime'].equals(s0_target_labels_df['frameTime'])\n",
    "        s0_s1_df = pd.concat([s0_feature_df,s1_feature_df],axis=1)     \n",
    "        assert not s0_s1_df.isnull().values.any()     \n",
    "        # Determine the number of sequences for this dialogue \n",
    "        num_sequences = int(np.floor(len(s0_feature_df.index))/self.num_context_frames)\n",
    "        for i in range(num_sequences):\n",
    "            x = np.asarray(s0_s1_df.loc[:,s0_s1_df.columns != 'frameTime'][i * \\\n",
    "                self.num_context_frames : (i * self.num_context_frames) \\\n",
    "                    + self.num_context_frames])\n",
    "            y = np.asarray(s0_target_labels_df.loc[:,s0_target_labels_df.columns\\\n",
    "                    != 'frameTime'][i * self.num_context_frames : \\\n",
    "                        (i * self.num_context_frames) + self.num_context_frames])[-1,:]\n",
    "            self.xs.append(x)\n",
    "            self.ys.append(y)\n",
    "\n",
    "    def __load_dataframes(self, dialogue):\n",
    "        if self.target_participant == \"f\":\n",
    "            s0_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"f\"], index_col=0,delimiter=\",\") \n",
    "            s1_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"g\"],index_col=0,delimiter=\",\")\n",
    "        else:\n",
    "            s0_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"g\"],index_col=0,delimiter=\",\") \n",
    "            s1_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"f\"],index_col=0,delimiter=\",\")\n",
    "        return s0_feature_df, s1_feature_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_dialogues(dataset_paths, test_size=0.25, val_size=0.2, \n",
    "        seed=GLOBAL_SEED):\n",
    "    dataset_paths = deepcopy(dataset_paths)\n",
    "    dialogue_names = sorted(list(set([get_maptask_dialogue(p) for p in dataset_paths])))\n",
    "    train_dialogues, test_dialogues = train_test_split(dialogue_names, \n",
    "        test_size=test_size,random_state=seed)\n",
    "    train_dialogues, val_dialogues = train_test_split(train_dialogues, \n",
    "        test_size=val_size,random_state=seed)\n",
    "    return train_dialogues, val_dialogues, test_dialogues \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapTask Dataclass and Method Definitions \n",
    "\n",
    "These are required to be in the same notebook to be loaded. \n",
    "\n",
    "\n",
    "NOTE: **DO NOT** modify these here, refer to 2.0-MU-Skantze-MapTask-Dataset-POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:Seed worker can be used to ensure reproducibility in DataLoader \n",
    "# across runs. \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed =GLOBAL_SEED\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def generate_dataloader(dataset, batch_size=32, shuffle=True, num_workers=0, \n",
    "        drop_last=True, pin_memory=True):\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        num_workers=num_workers, \n",
    "        drop_last=drop_last, # We always want to remove the last incomplete batch. \n",
    "        pin_memory=pin_memory, \n",
    "        worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skantze 2017 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skantze2017LSTMPredictor(nn.Module):\n",
    "    # TODO: The seed parameter is used to make sure that the layer initializations \n",
    "    # are the same across runs for model comparison - this should probably be \n",
    "    # either refactored or replaced.  \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_dim=1, seed=None):\n",
    "        super().__init__()\n",
    "        # Defining the vars. \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.output_dim = output_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        # LSTM Layers\n",
    "        if seed != None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim,layer_dim,batch_first=True) #  tanh activation is the default. \n",
    "        # Fully connected layer\n",
    "        if seed != None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.fc = nn.Linear(hidden_dim,output_dim,bias=True)\n",
    "        if seed != None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.fc_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the hidden states for first input with zeroes. \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Initialize the cell state for the first input with zeroes. \n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0),self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # TODO: Still need to implement TBPTT in the model for the last 10 seconds, \n",
    "        # which is what was done in the Skantze paper. \n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x.float(),(h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :] # Because we need the final output. \n",
    "        return self.fc_activation(self.fc(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Training Utility Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing training loop for a single batch \n",
    "\n",
    "def train_step(model,optimizer, loss_fn, x, y):\n",
    "    # Make sure X type is correct. \n",
    "    x = x.type(torch.FloatTensor)\n",
    "    # NOTE: The input should not have any nans. \n",
    "    assert not torch.isnan(x).any()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x)\n",
    "    assert not torch.isnan(y_hat).any()\n",
    "    assert not torch.isnan(y).any()\n",
    "    loss = loss_fn(y,y_hat)\n",
    "    assert not torch.isnan(y).any()\n",
    "    loss.backward()\n",
    "    # nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def validate_step(model, loss_fn, x,y):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y,y_hat).item()\n",
    "        return loss \n",
    "\n",
    "def next_batch(dataloader):\n",
    "    # Loop over dataset \n",
    "    for x_batch ,y_batch in dataloader:\n",
    "        yield x_batch, y_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer, loss_fn,train_dataloader, val_dataloader, n_epochs,\n",
    "        validate_n_epochs=1,save_n_epochs=None, save_fn = None,\n",
    "        print_n_epochs=None):\n",
    "    # Set the model to training \n",
    "    model.train()\n",
    "\n",
    "    # Vars. to record results across training \n",
    "    training_losses = []\n",
    "    validation_losses = [] \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0 \n",
    "        samples = 0.0 \n",
    "\n",
    "        # Run the training batch \n",
    "        for batch_x, batch_y in next_batch(train_dataloader):\n",
    "            loss = train_step(model, optimizer,loss_fn,batch_x, batch_y)\n",
    "            # -- Updates loss etc. \n",
    "            train_loss += loss.item() * batch_y.size(0) \n",
    "            # TODO: Not sure if this is the correct way to get accuracy. \n",
    "            samples += batch_y.size(0)\n",
    "        training_losses.append(train_loss / samples)\n",
    "\n",
    "        # Run the validation batch\n",
    "        if epoch % validate_n_epochs == 0:\n",
    "            batch_val_losses = []\n",
    "            for batch_x, batch_y in next_batch(val_dataloader):\n",
    "                loss = validate_step(model, loss_fn,batch_x,batch_y)\n",
    "                batch_val_losses.append(loss) \n",
    "            validation_losses.append(np.mean(batch_val_losses))\n",
    "        else:\n",
    "            batch_val_losses = 0.0\n",
    "        \n",
    "        # Save the model if required. \n",
    "        if (save_n_epochs != None and epoch % save_n_epochs == 0 and save_fn != None) or \\\n",
    "                (save_fn != None and epoch == n_epochs):\n",
    "            info = {\n",
    "                \"epoch\" : epoch, \n",
    "                \"loss\" : train_loss/samples,\n",
    "                \"val_loss\" : np.mean(batch_val_losses)}\n",
    "            save_fn(model ,info)\n",
    "        # Print if needed \n",
    "        if print_n_epochs != None and epoch % print_n_epochs == 0:\n",
    "            print(f\"[{epoch}/{n_epochs}] Training loss: {train_loss/samples:.4f}\\t \\\n",
    "                Validation loss: {np.mean(batch_val_losses):.4f}\")\n",
    "\n",
    "    return training_losses, validation_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skantze2017 Model Specific Training Utility Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Methods to save and load this specific model. \n",
    "\n",
    "import h5py \n",
    "\n",
    "def save_for_inference(save_path):\n",
    "    assert not os.path.isfile(save_path)\n",
    "    return lambda model, info: (\n",
    "        torch.save({\n",
    "            \"kwargs\" : {\n",
    "                \"input_dim\" : model.input_dim, \n",
    "                \"hidden_dim\" : model.hidden_dim,\n",
    "                \"output_dim\" : model.output_dim,\n",
    "                \"layer_dim\" : model.layer_dim}, \n",
    "            \"info\" : deepcopy(info),\n",
    "            \"model_state_dict\" : model.state_dict()}, save_path))\n",
    "    \n",
    "def load_for_inference(save_file_path):\n",
    "    assert os.path.isfile(save_file_path) \n",
    "    checkpoint = torch.load(save_file_path)\n",
    "    model = Skantze2017LSTMPredictor(**checkpoint[\"kwargs\"])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model, checkpoint['info']\n",
    "\n",
    "\n",
    "def save_losses(save_dir, filename):\n",
    "    assert os.path.isdir(save_dir)\n",
    "    path = \"{}/{}.h5\".format(save_dir,filename)\n",
    "    hf : h5py.File = h5py.File(path,'w')\n",
    "\n",
    "    def h5_save_losses(grp_name, losses_dict):\n",
    "        grp = hf.create_group(grp_name)\n",
    "        for loss_name, losses in losses_dict.items():\n",
    "            # loss_name = np.asarray(losses) \n",
    "            grp.create_dataset(loss_name, data=losses)\n",
    "        \n",
    "    return hf, path, h5_save_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skantze2017 Model Specific Testing Utility Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_skantze2017(model, test_dataloader):\n",
    "    loss_fn = nn.L1Loss()\n",
    "    losses = [] \n",
    "    va_losses = []\n",
    "    with torch.no_grad():\n",
    "        model.eval() \n",
    "        for x_batch, y_batch in next_batch(test_dataloader):\n",
    "            x_batch = x_batch.view([1, -1, x_batch.size(-1)])\n",
    "            y_hat = model(x_batch)\n",
    "            loss = loss_fn(y_hat, y_batch)\n",
    "            losses.append(loss) \n",
    "            nonzero_target_indices = np.nonzero(y_batch ==1)[:,1]\n",
    "            if len(nonzero_target_indices) > 0:\n",
    "                va_loss = loss_fn(\n",
    "                    y_hat[:,nonzero_target_indices], y_batch[:,nonzero_target_indices])\n",
    "                va_losses.append(va_loss)\n",
    "        return losses , va_losses\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Dataset Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skantze2017PausesDataset(Dataset):\n",
    "\n",
    "    HOLD_LABEL = 0 \n",
    "    SHIFT_LABEL = 1 \n",
    "\n",
    "    def __init__(self, feature_paths_map , sequence_length_ms, min_pause_length_ms, \n",
    "                max_future_silence_window_ms, s0_participant, frame_step_size_ms, \n",
    "                save_dir = None):\n",
    "        # Params . \n",
    "        self.feature_paths_map = feature_paths_map \n",
    "        self.sequence_length_ms = sequence_length_ms \n",
    "        self.min_pause_length_ms = min_pause_length_ms\n",
    "        self.max_future_silence_window_ms = max_future_silence_window_ms\n",
    "        self.s0_participant = s0_participant\n",
    "        self.frame_step_size_ms = frame_step_size_ms  \n",
    "        self.save_dir = save_dir # If the save dir is provided, saves the dataset as it is built. \n",
    "        # Calculated \n",
    "        self.num_context_frames = int(sequence_length_ms / frame_step_size_ms)\n",
    "        self.num_pause_frames = int(min_pause_length_ms / frame_step_size_ms)\n",
    "        self.future_window_frames = int(max_future_silence_window_ms / frame_step_size_ms)\n",
    "        # Data Storage vars. \n",
    "        self.xs = [] \n",
    "        self.ys = [] \n",
    "        self.num_silences = 0 \n",
    "        self.num_holds = 0 \n",
    "        self.num_shifts = 0 \n",
    "        self.num_pauses = 0 \n",
    "        # Prepare the data \n",
    "        for dialogue in list(self.feature_paths_map.keys()):\n",
    "            self.__prepare_items(dialogue, self.__prepare_pauses_df(dialogue))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx > self.__len__():\n",
    "            raise Exception \n",
    "        # NOTE: xs has target speaker features concatenated with non-target speaker features. \n",
    "        # ys is the previous speaker, hold / shift label, and the next speaker. \n",
    "        return self.xs[idx], self.ys[idx]\n",
    "    \n",
    "    def get_pause_statistics(self):\n",
    "        return {\n",
    "            \"min_pause_length_ms \" : self.min_pause_length_ms , \n",
    "            \"sequence_length_ms\" : self.sequence_length_ms, \n",
    "            \"max_future_silence_window_ms\" : self.max_future_silence_window_ms, \n",
    "            \"num_silences\" : self.num_silences, \n",
    "            \"num_holds\" : self.num_holds, \n",
    "            \"num_shifts\" : self.num_shifts,\n",
    "            \"num_pauses\" : self.num_pauses,\n",
    "            \"s0_participant\" : self.s0_participant\n",
    "        }\n",
    "    \n",
    "    def __prepare_items(self,dialogue,pauses_df):\n",
    "        s0_feature_df, s1_feature_df = self.__load_dataframes(dialogue)\n",
    "        # Collect the data for both models\n",
    "        s0_s1_df = pd.concat([s0_feature_df.loc[:,s0_feature_df.columns != 'frameTime'],s1_feature_df.loc[:,s1_feature_df.columns != 'frameTime']],axis=1)   \n",
    "        s1_s0_df = pd.concat([s1_feature_df.loc[:,s1_feature_df.columns != 'frameTime'],s0_feature_df.loc[:,s0_feature_df.columns != 'frameTime']],axis=1) \n",
    "        for pause_data in pauses_df.itertuples():\n",
    "            _,_, previous_speaker, idx_after_silence_frames, _, hold_shift_label, next_speaker = pause_data \n",
    "            idx_after_silence_frames = int(idx_after_silence_frames)\n",
    "            # Collect features for each speaker equal to sequence length / num context frames. \n",
    "            if idx_after_silence_frames- self.num_context_frames >= 0:\n",
    "                x_s0 = np.asarray(s0_s1_df.iloc[idx_after_silence_frames-self.num_context_frames:idx_after_silence_frames])\n",
    "                x_s1 = np.asarray(s1_s0_df.iloc[idx_after_silence_frames-self.num_context_frames:idx_after_silence_frames])\n",
    "            else:\n",
    "                num_pad = self.num_context_frames - idx_after_silence_frames -1 \n",
    "                x_s0 = np.pad(np.asarray(s0_s1_df.iloc[0:idx_after_silence_frames+1]),[(num_pad,0),(0,0)],'constant')\n",
    "                x_s1 = np.pad(np.asarray(s1_s0_df.iloc[0:idx_after_silence_frames+1]),[(num_pad,0),(0,0)],'constant')\n",
    "            self.xs.append((x_s0,x_s1)) \n",
    "            self.ys.append((int(previous_speaker), int(hold_shift_label), int(next_speaker)))\n",
    "\n",
    "    def __prepare_pauses_df(self, dialogue):\n",
    "        s0_feature_df, s1_feature_df = self.__load_dataframes(dialogue)\n",
    "        # Obtain frame indices where both speakers are speaking. \n",
    "        s0_va_idxs =  np.where(s0_feature_df['voiceActivity'] == 1)[0]\n",
    "        s1_va_idxs =  np.where(s1_feature_df['voiceActivity'] == 1)[0]\n",
    "        va_idxs = np.union1d(s0_va_idxs,s1_va_idxs)\n",
    "        # Obtain index of last speaking frame before silences \n",
    "        speak_before_silence_frames_idx = \\\n",
    "            va_idxs[np.where(np.diff(va_idxs) > self.num_pause_frames)]\n",
    "        self.num_silences += len(speak_before_silence_frames_idx)\n",
    "        # Remove scenarios where both speakers were speaking last i.e., only \n",
    "        # one speaking could have been speaking before te pause\n",
    "        speak_before_silence_frames_idx = [idx for idx in speak_before_silence_frames_idx \\\n",
    "            if not (idx in s0_va_idxs and idx in s1_va_idxs) and (idx in s0_va_idxs or idx in s1_va_idxs)]\n",
    "        # Next, we want to find all the instances where one (and only one) \n",
    "        # speaker continues within the next future_window_ms seconds. \n",
    "        pauses_df = pd.DataFrame(columns=['pauseStartFrameTime', 'previousSpeaker',\n",
    "            'pauseEndFrameIndex', 'nextSpeechFrameIndex', 'holdShiftLabel', 'nextSpeaker'])\n",
    "        for i,idx in enumerate(speak_before_silence_frames_idx):\n",
    "            # Obtain index of the frame after the specified pause length. \n",
    "            idx_after_silence_frames = idx + self.num_pause_frames + 1 \n",
    "            # Get the voice activity in the specified future window\n",
    "            s0_window_va = np.asarray((s0_feature_df['voiceActivity'])[\n",
    "                idx_after_silence_frames:idx_after_silence_frames+self.future_window_frames] == 1)\n",
    "            s1_window_va = np.asarray((s1_feature_df['voiceActivity'])[\n",
    "                idx_after_silence_frames:idx_after_silence_frames+self.future_window_frames] == 1)\n",
    "            # Determine the last speaker before silence \n",
    "            last_participant = 0 if s0_feature_df['voiceActivity'].iloc[idx] else 1 \n",
    "            # NOTE: Both speakers might start speaking in the future window but we want \n",
    "            # to make sure that only one of the speakers starts i.e., no overlap. \n",
    "            # NOTE: 0 = hold, 1 = shift \n",
    "            # Condition 1: Speaker 0 is next. \n",
    "            if s0_window_va.any() and not s1_window_va.any():\n",
    "                next_va_idx = np.argmax(s0_window_va) + idx_after_silence_frames  \n",
    "                hold_shift_label = self.HOLD_LABEL if last_participant == 0 else self.SHIFT_LABEL\n",
    "                pauses_df.loc[i] = (\n",
    "                    s0_feature_df.iloc[idx]['frameTime'], last_participant,\n",
    "                     idx_after_silence_frames, next_va_idx, hold_shift_label,0)             \n",
    "            # Condition 2: Speaker 1 is next. \n",
    "            elif s1_window_va.any() and not s0_window_va.any():\n",
    "                next_va_idx = np.argmax(s1_window_va) + idx_after_silence_frames \n",
    "                hold_shift_label = self.HOLD_LABEL if last_participant == 1 else self.SHIFT_LABEL\n",
    "                pauses_df.loc[i] = (\n",
    "                    s1_feature_df.iloc[idx]['frameTime'], last_participant,\n",
    "                    idx_after_silence_frames, next_va_idx, hold_shift_label,1) \n",
    "        # Update the shift / hold values \n",
    "        self.num_holds += (pauses_df['holdShiftLabel'] == self.HOLD_LABEL).sum()\n",
    "        self.num_shifts += (pauses_df['holdShiftLabel'] == self.SHIFT_LABEL).sum()\n",
    "        self.num_pauses = self.num_holds + self.num_shifts\n",
    "        # Save the pauses df if save dir provided \n",
    "        if self.save_dir != None:\n",
    "            pauses_df.to_csv(\"{}/{}_pauses_df.csv\".format(self.save_dir, dialogue))\n",
    "        return pauses_df \n",
    "\n",
    "    def __load_dataframes(self, dialogue):\n",
    "        if self.s0_participant == \"f\":\n",
    "            s0_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"f\"], index_col=0,delimiter=\",\") \n",
    "            s1_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"g\"],index_col=0,delimiter=\",\")\n",
    "        else:\n",
    "            s0_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"g\"],index_col=0,delimiter=\",\") \n",
    "            s1_feature_df = pd.read_csv(self.feature_paths_map[dialogue][\"f\"],index_col=0,delimiter=\",\")\n",
    "        # Trim the dataframes to the same length \n",
    "        min_num_frames = np.min([len(s0_feature_df.index),len(s1_feature_df.index)])\n",
    "        s0_feature_df = s0_feature_df[:min_num_frames]\n",
    "        s1_feature_df = s1_feature_df[:min_num_frames]\n",
    "        assert len(s0_feature_df) == len(s1_feature_df)\n",
    "        return s0_feature_df, s1_feature_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.2: Prediction at Pauses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we want to make HOLD / SHIFT label predictions at pauses using f and g speaker models trained for the same number of epochs and on the same feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the experiment \n",
    "MODEL_NAME = \"skantze2017\"\n",
    "FEATURE_SET = \"full\"\n",
    "FEATURE_SET_DIR = FULL_PROCESSED_FEATURE_DIR if FEATURE_SET == \"full\" else PROSODY_PROCESSED_FEATURE_DIR\n",
    "\n",
    "# Model params. \n",
    "NUM_HIDDEN_NODES = 40 \n",
    "SEQUENCE_LENGTHS_MS = 60_000\n",
    "PREDICTION_LENGTHS_MS = 1000\n",
    "FRAME_STEP_SIZE_MS = 50 \n",
    "\n",
    "# Pause Dataset Params. \n",
    "MIN_PAUSE_LENGTH_MS = 500 \n",
    "MAX_FUTURE_SILENCE_WINDOW_MS = 1000 \n",
    "S0_PARTICIPANT = \"f\"\n",
    "PAUSE_DATASET_SAVE_DIR = os.path.join(\n",
    "    PROCESSED_DATA_DIR,\"pause_dataset_{}\".format(FEATURE_SET))\n",
    "\n",
    "# Training args. \n",
    "PRINT_N_EPOCHS = 1 \n",
    "SAVE_N_EPOCHS = 1 \n",
    "LEARNING_RATE = 0.01 \n",
    "WEIGHT_DECAY = 0.001\n",
    "N_EPOCHS = 60\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Appropriate Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This method needs to be redefined for experiment 4.2 \n",
    "def save_for_inference(save_dir,model_name):\n",
    "    assert os.path.isdir(save_dir)\n",
    "    return lambda model, info: (\n",
    "        torch.save({\n",
    "            \"kwargs\" : {\n",
    "                \"input_dim\" : model.input_dim, \n",
    "                \"hidden_dim\" : model.hidden_dim,\n",
    "                \"output_dim\" : model.output_dim,\n",
    "                \"layer_dim\" : model.layer_dim}, \n",
    "            \"info\" : deepcopy(info),\n",
    "            \"model_state_dict\" : model.state_dict()}, \n",
    "            os.path.join(save_dir, \"{}_{}.pt\".format(model_name, info['epoch']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dialogue splits \n",
    "dataset_csv_paths =  glob.glob(\"{}/*.csv\".format(FEATURE_SET_DIR))\n",
    "LIMITED_RESOURCES = False # TODO: Temporarily setting this to True for testing - remove later. \n",
    "if LIMITED_RESOURCES:\n",
    "    dataset_csv_paths = dataset_csv_paths[:SMALL_DATASET_SIZE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 20 32\n"
     ]
    }
   ],
   "source": [
    "train_dialogues, val_dialogues, test_dialogues = \\\n",
    "    get_train_val_test_dialogues(dataset_csv_paths)\n",
    "feature_paths_map =  \\\n",
    "    collect_dialogue_features(train_dialogues,FEATURE_SET_DIR)\n",
    "print(len(train_dialogues), len(val_dialogues), len(test_dialogues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the save dir / func for this config. \n",
    "exp_config_name =  \"{}_{}_{}\".format(SEQUENCE_LENGTHS_MS,\n",
    "            PREDICTION_LENGTHS_MS,FEATURE_SET)\n",
    "save_models_dir = os.path.join(SAVE_MODELS_DIR,exp_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target speaker: f\n",
      "Generating training datasets...\n",
      "Generating training dataloaders...\n",
      "Initializing model...\n",
      "Evaluating and saving the untrained model for reference...\n",
      "Training models...\n",
      "[1/60] Training loss: 0.2216\t                 Validation loss: 0.2083\n",
      "[2/60] Training loss: 0.2084\t                 Validation loss: 0.2074\n",
      "[3/60] Training loss: 0.2144\t                 Validation loss: 0.1844\n",
      "[4/60] Training loss: 0.2087\t                 Validation loss: 0.1999\n",
      "[5/60] Training loss: 0.1993\t                 Validation loss: 0.1999\n",
      "[6/60] Training loss: 0.2061\t                 Validation loss: 0.1876\n",
      "[7/60] Training loss: 0.2001\t                 Validation loss: 0.1929\n",
      "[8/60] Training loss: 0.2105\t                 Validation loss: 0.2236\n",
      "[9/60] Training loss: 0.2129\t                 Validation loss: 0.1840\n",
      "[10/60] Training loss: 0.1954\t                 Validation loss: 0.1991\n",
      "[11/60] Training loss: 0.2016\t                 Validation loss: 0.1781\n",
      "[12/60] Training loss: 0.2045\t                 Validation loss: 0.1784\n",
      "[13/60] Training loss: 0.2036\t                 Validation loss: 0.2065\n",
      "[14/60] Training loss: 0.2049\t                 Validation loss: 0.1864\n",
      "[15/60] Training loss: 0.2031\t                 Validation loss: 0.1865\n",
      "[16/60] Training loss: 0.2042\t                 Validation loss: 0.2076\n",
      "[17/60] Training loss: 0.2094\t                 Validation loss: 0.2073\n",
      "[18/60] Training loss: 0.2072\t                 Validation loss: 0.1973\n",
      "[19/60] Training loss: 0.2024\t                 Validation loss: 0.1931\n",
      "[20/60] Training loss: 0.2065\t                 Validation loss: 0.1995\n",
      "[21/60] Training loss: 0.2091\t                 Validation loss: 0.2003\n",
      "[22/60] Training loss: 0.2007\t                 Validation loss: 0.1980\n",
      "[23/60] Training loss: 0.2114\t                 Validation loss: 0.2408\n",
      "[24/60] Training loss: 0.2001\t                 Validation loss: 0.1840\n",
      "[25/60] Training loss: 0.2013\t                 Validation loss: 0.1982\n",
      "[26/60] Training loss: 0.1990\t                 Validation loss: 0.1937\n",
      "[27/60] Training loss: 0.2016\t                 Validation loss: 0.2012\n",
      "[28/60] Training loss: 0.2052\t                 Validation loss: 0.2076\n",
      "[29/60] Training loss: 0.2015\t                 Validation loss: 0.2150\n",
      "[30/60] Training loss: 0.2099\t                 Validation loss: 0.1893\n",
      "[31/60] Training loss: 0.2053\t                 Validation loss: 0.1848\n",
      "[32/60] Training loss: 0.1985\t                 Validation loss: 0.1708\n",
      "[33/60] Training loss: 0.2041\t                 Validation loss: 0.1749\n",
      "[34/60] Training loss: 0.2011\t                 Validation loss: 0.1928\n",
      "[35/60] Training loss: 0.2009\t                 Validation loss: 0.2077\n",
      "[36/60] Training loss: 0.2072\t                 Validation loss: 0.1823\n",
      "[37/60] Training loss: 0.2008\t                 Validation loss: 0.1791\n",
      "[38/60] Training loss: 0.1969\t                 Validation loss: 0.2048\n",
      "[39/60] Training loss: 0.2004\t                 Validation loss: 0.1807\n",
      "[40/60] Training loss: 0.2009\t                 Validation loss: 0.1800\n",
      "[41/60] Training loss: 0.2011\t                 Validation loss: 0.1919\n",
      "[42/60] Training loss: 0.2098\t                 Validation loss: 0.2090\n",
      "[43/60] Training loss: 0.2070\t                 Validation loss: 0.1943\n",
      "[44/60] Training loss: 0.2056\t                 Validation loss: 0.2290\n",
      "[45/60] Training loss: 0.2117\t                 Validation loss: 0.2031\n",
      "[46/60] Training loss: 0.2021\t                 Validation loss: 0.1907\n",
      "[47/60] Training loss: 0.2056\t                 Validation loss: 0.1976\n",
      "[48/60] Training loss: 0.2090\t                 Validation loss: 0.2111\n",
      "[49/60] Training loss: 0.2082\t                 Validation loss: 0.2009\n",
      "[50/60] Training loss: 0.2098\t                 Validation loss: 0.1864\n",
      "[51/60] Training loss: 0.2015\t                 Validation loss: 0.2123\n",
      "[52/60] Training loss: 0.2082\t                 Validation loss: 0.1830\n",
      "[53/60] Training loss: 0.2002\t                 Validation loss: 0.2027\n",
      "[54/60] Training loss: 0.2018\t                 Validation loss: 0.1946\n",
      "[55/60] Training loss: 0.2042\t                 Validation loss: 0.1865\n",
      "[56/60] Training loss: 0.2035\t                 Validation loss: 0.1947\n",
      "[57/60] Training loss: 0.2086\t                 Validation loss: 0.1921\n",
      "[58/60] Training loss: 0.2030\t                 Validation loss: 0.2081\n",
      "[59/60] Training loss: 0.2012\t                 Validation loss: 0.1844\n",
      "[60/60] Training loss: 0.2075\t                 Validation loss: 0.1833\n",
      "Evaluating models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target speaker: g\n",
      "Generating training datasets...\n",
      "Generating training dataloaders...\n",
      "Initializing model...\n",
      "Evaluating and saving the untrained model for reference...\n",
      "Training models...\n",
      "[1/60] Training loss: 0.3599\t                 Validation loss: 0.3073\n",
      "[2/60] Training loss: 0.3144\t                 Validation loss: 0.3872\n",
      "[3/60] Training loss: 0.3351\t                 Validation loss: 0.4660\n",
      "[4/60] Training loss: 0.3408\t                 Validation loss: 0.3062\n",
      "[5/60] Training loss: 0.3084\t                 Validation loss: 0.2974\n",
      "[6/60] Training loss: 0.3226\t                 Validation loss: 0.3080\n",
      "[7/60] Training loss: 0.3372\t                 Validation loss: 0.3300\n",
      "[8/60] Training loss: 0.3156\t                 Validation loss: 0.3684\n",
      "[9/60] Training loss: 0.3233\t                 Validation loss: 0.3171\n",
      "[10/60] Training loss: 0.3160\t                 Validation loss: 0.3463\n",
      "[11/60] Training loss: 0.3201\t                 Validation loss: 0.2989\n",
      "[12/60] Training loss: 0.3175\t                 Validation loss: 0.2830\n",
      "[13/60] Training loss: 0.3114\t                 Validation loss: 0.3185\n",
      "[14/60] Training loss: 0.3140\t                 Validation loss: 0.2987\n",
      "[15/60] Training loss: 0.3076\t                 Validation loss: 0.3168\n",
      "[16/60] Training loss: 0.3159\t                 Validation loss: 0.3171\n",
      "[17/60] Training loss: 0.3129\t                 Validation loss: 0.3186\n",
      "[18/60] Training loss: 0.3145\t                 Validation loss: 0.3075\n",
      "[19/60] Training loss: 0.3097\t                 Validation loss: 0.3613\n",
      "[20/60] Training loss: 0.3103\t                 Validation loss: 0.3050\n",
      "[21/60] Training loss: 0.3071\t                 Validation loss: 0.3087\n",
      "[22/60] Training loss: 0.3129\t                 Validation loss: 0.3061\n",
      "[23/60] Training loss: 0.3255\t                 Validation loss: 0.3152\n",
      "[24/60] Training loss: 0.3073\t                 Validation loss: 0.3024\n",
      "[25/60] Training loss: 0.3157\t                 Validation loss: 0.2953\n",
      "[26/60] Training loss: 0.3084\t                 Validation loss: 0.3646\n",
      "[27/60] Training loss: 0.3122\t                 Validation loss: 0.3075\n",
      "[28/60] Training loss: 0.3246\t                 Validation loss: 0.4126\n",
      "[29/60] Training loss: 0.3136\t                 Validation loss: 0.2912\n",
      "[30/60] Training loss: 0.3224\t                 Validation loss: 0.2986\n",
      "[31/60] Training loss: 0.3027\t                 Validation loss: 0.3100\n",
      "[32/60] Training loss: 0.3143\t                 Validation loss: 0.3216\n",
      "[33/60] Training loss: 0.3171\t                 Validation loss: 0.3659\n",
      "[34/60] Training loss: 0.3151\t                 Validation loss: 0.3041\n",
      "[35/60] Training loss: 0.3132\t                 Validation loss: 0.3235\n",
      "[36/60] Training loss: 0.3483\t                 Validation loss: 0.3168\n",
      "[37/60] Training loss: 0.3097\t                 Validation loss: 0.2924\n",
      "[38/60] Training loss: 0.3157\t                 Validation loss: 0.3307\n",
      "[39/60] Training loss: 0.3033\t                 Validation loss: 0.3346\n",
      "[40/60] Training loss: 0.3161\t                 Validation loss: 0.3252\n",
      "[41/60] Training loss: 0.3601\t                 Validation loss: 0.3453\n",
      "[42/60] Training loss: 0.3165\t                 Validation loss: 0.4185\n",
      "[43/60] Training loss: 0.3456\t                 Validation loss: 0.3485\n",
      "[44/60] Training loss: 0.3043\t                 Validation loss: 0.3023\n",
      "[45/60] Training loss: 0.3087\t                 Validation loss: 0.3314\n",
      "[46/60] Training loss: 0.3151\t                 Validation loss: 0.3090\n",
      "[47/60] Training loss: 0.3238\t                 Validation loss: 0.3015\n",
      "[48/60] Training loss: 0.3109\t                 Validation loss: 0.3668\n",
      "[49/60] Training loss: 0.3302\t                 Validation loss: 0.3244\n",
      "[50/60] Training loss: 0.3135\t                 Validation loss: 0.3205\n",
      "[51/60] Training loss: 0.3105\t                 Validation loss: 0.2958\n",
      "[52/60] Training loss: 0.3203\t                 Validation loss: 0.3037\n",
      "[53/60] Training loss: 0.3192\t                 Validation loss: 0.3196\n",
      "[54/60] Training loss: 0.3222\t                 Validation loss: 0.2993\n",
      "[55/60] Training loss: 0.3089\t                 Validation loss: 0.3436\n",
      "[56/60] Training loss: 0.3087\t                 Validation loss: 0.3162\n",
      "[57/60] Training loss: 0.3128\t                 Validation loss: 0.3016\n",
      "[58/60] Training loss: 0.3312\t                 Validation loss: 0.3363\n",
      "[59/60] Training loss: 0.3272\t                 Validation loss: 0.3042\n",
      "[60/60] Training loss: 0.3240\t                 Validation loss: 0.3085\n",
      "Evaluating models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# -- First step is to train the models and save them at every epoch. \n",
    "\n",
    "\n",
    "os.makedirs(save_models_dir,exist_ok=True)\n",
    "# Creating the loss save function \n",
    "hf, hf_save_path, save_loss_fn= save_losses(save_models_dir, \"exp_4.2\")\n",
    "# Starting training per speaker \n",
    "for target_speaker in (\"f\",\"g\"):\n",
    "    print(\"Target speaker: {}\".format(target_speaker))\n",
    "    # Make the save dirs. / method for this target speaker \n",
    "    speaker_save_dir = os.path.join(save_models_dir,target_speaker)\n",
    "    os.makedirs(speaker_save_dir,exist_ok=True)\n",
    "    save_fn = save_for_inference(speaker_save_dir,MODEL_NAME)\n",
    "    print(\"Generating training datasets...\")\n",
    "    datasets = [] \n",
    "    for dialogues in (train_dialogues, val_dialogues, test_dialogues):\n",
    "        dataset = Skantze2017VAPredictionMapTaskDataset(\n",
    "            feature_paths_map=collect_dialogue_features(\n",
    "                                    dialogues,FEATURE_SET_DIR), \n",
    "            sequence_length_ms=SEQUENCE_LENGTHS_MS, \n",
    "            prediction_length_ms=PREDICTION_LENGTHS_MS, \n",
    "            target_participant=target_speaker, \n",
    "            frame_step_size_ms=FRAME_STEP_SIZE_MS)\n",
    "        datasets.append(deepcopy(dataset))\n",
    "    print(\"Generating training dataloaders...\")\n",
    "    dataloaders = [] \n",
    "    for i, (dataset_name, batch_size) in enumerate(\n",
    "        zip(('train','val','test'),(1,1,1))):\n",
    "        dataloader = generate_dataloader(datasets[i],batch_size=batch_size)\n",
    "        dataloaders.append(dataloader)\n",
    "    print(\"Initializing model...\")\n",
    "    model = Skantze2017LSTMPredictor(\n",
    "        input_dim=next(iter(datasets[0]))[0].shape[-1],\n",
    "        hidden_dim=NUM_HIDDEN_NODES , \n",
    "        output_dim=int(PREDICTION_LENGTHS_MS/FRAME_STEP_SIZE_MS))\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(),lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n",
    "    # Save the untrained model for reference \n",
    "    print(\"Evaluating and saving the untrained model for reference...\")\n",
    "    per_epoch_eval_losses = []\n",
    "    l1_losses, _ = evaluate_skantze2017(model, dataloaders[-1])\n",
    "    per_epoch_eval_losses.append(np.mean(l1_losses))\n",
    "    save_fn(model,{ \"epoch\" : 0,  \"loss\" : 0.0,  \"val_loss\" : 0.0})\n",
    "    # -- Start training \n",
    "    print(\"Training models...\")\n",
    "    training_losses, val_losses = train(\n",
    "        model,optimizer, loss_fn,dataloaders[0], dataloaders[1], \n",
    "        n_epochs=N_EPOCHS, print_n_epochs=PRINT_N_EPOCHS,\n",
    "        save_fn=save_fn,save_n_epochs=SAVE_N_EPOCHS)\n",
    "    # -- Evaluate the models \n",
    "    # The evaluation is the average evaluation loss for models trained \n",
    "    # utpo epoch N. \n",
    "    print(\"Evaluating models...\")\n",
    "    model_paths = sorted(glob.glob(\"{}/*.pt\".format(save_models_dir))) \n",
    "    for model_path in tqdm(model_paths):\n",
    "        model, info = load_for_inference(model_path)\n",
    "        model.eval()\n",
    "        l1_losses, _ = evaluate_skantze2017(model, dataloaders[-1])\n",
    "        per_epoch_eval_losses.append(np.mean(l1_losses))\n",
    "    # Save losses for the final epoch of this configuration.\n",
    "    # NOTE: These losses are the per epoch loss for every epoch. \n",
    "    save_loss_fn(\n",
    "        \"{}_{}_{}\".format(exp_config_name,target_speaker,N_EPOCHS), {\n",
    "            \"per_epoch_training\" : training_losses, \n",
    "            \"per_epoch_validation\" : val_losses, \n",
    "            \"per_epoch_eval\" : per_epoch_eval_losses})\n",
    "hf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the Pause Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the models \n",
    "models = defaultdict(lambda : list())\n",
    "for target_speaker in (\"f\",\"g\"):\n",
    "    speaker_save_dir = os.path.join(save_models_dir,target_speaker)\n",
    "    model_paths = glob.glob(\"{}/*.pt\".format(speaker_save_dir))\n",
    "    models[target_speaker] = sorted(model_paths)\n",
    "assert len(models[\"f\"]) == len(models[\"g\"])\n",
    "# Collect the s0 and s1 models \n",
    "if S0_PARTICIPANT == \"f\":\n",
    "    s0_models = models[\"f\"]\n",
    "    s1_models = models[\"g\"]\n",
    "else:\n",
    "    s0_models = models[\"g\"]\n",
    "    s1_models = models[\"f\"]\n",
    "assert len(s0_models) == len(s1_models)\n",
    "# TODO: The models are not being sorted properly before prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_0.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_0.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_1.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_1.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_2.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_2.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_3.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_3.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_4.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_4.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_5.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_5.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_6.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_6.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_7.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_7.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_8.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_8.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_9.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_9.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_10.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_10.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_11.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_11.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_12.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_12.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_13.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_13.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_14.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_14.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_15.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_15.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_16.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_16.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_17.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_17.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_18.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_18.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_19.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_19.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_20.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_20.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_21.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_21.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_22.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_22.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_23.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_23.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_24.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_24.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_25.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_25.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_26.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_26.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_27.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_27.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_28.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_28.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_29.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_29.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_30.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_30.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_31.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_31.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_32.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_32.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_33.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_33.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_34.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_34.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_35.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_35.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_36.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_36.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_37.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_37.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_38.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_38.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_39.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_39.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_40.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_40.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_41.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_41.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_42.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_42.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_43.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_43.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_44.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_44.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_45.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_45.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_46.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_46.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_47.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_47.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_48.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_48.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_49.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_49.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_50.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_50.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_51.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_51.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_52.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_52.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_53.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_53.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_54.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_54.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_55.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_55.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_56.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_56.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_57.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_57.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_58.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_58.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_59.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_59.pt'),\n",
       " ('/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/f/skantze2017_60.pt',\n",
       "  '/Users/muhammadumair/Documents/Repositories/mumair01-repos/TRP-Modeling/skantze_2017_continuous/models/skantze2017_exp_4_2_poc/60000_1000_full/g/skantze2017_60.pt')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to make sure we use the models from the lowest number of trained epochs to the highest number. \n",
    "s0_models.sort(key=lambda path :int(os.path.basename(path).split(\".\")[0].split(\"_\")[1]))\n",
    "s1_models.sort(key=lambda path :int(os.path.basename(path).split(\".\")[0].split(\"_\")[1]))\n",
    "list(zip(s0_models, s1_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the test dialogues \n",
    "# TODO: Only using the first two dialogues of the test set, which is a \n",
    "# hacky way of doing this for now. \n",
    "feature_paths_map =  \\\n",
    "    collect_dialogue_features(test_dialogues[:10],FEATURE_SET_DIR)\n",
    "\n",
    "pauses_save_dir = os.path.join(PROCESSED_DATA_DIR,\"pauses_dfs\")\n",
    "os.makedirs(pauses_save_dir,exist_ok=True)\n",
    "\n",
    "# Load the pause dataset \n",
    "test_pauses_dataset = Skantze2017PausesDataset(\n",
    "    feature_paths_map=feature_paths_map,\n",
    "    sequence_length_ms= SEQUENCE_LENGTHS_MS, \n",
    "    min_pause_length_ms=MIN_PAUSE_LENGTH_MS,  \n",
    "    max_future_silence_window_ms=MAX_FUTURE_SILENCE_WINDOW_MS, \n",
    "    s0_participant=S0_PARTICIPANT,\n",
    "    frame_step_size_ms=FRAME_STEP_SIZE_MS,\n",
    "    save_dir = pauses_save_dir)\n",
    "\n",
    "test_pauses_dataset.get_pause_statistics()\n",
    "\n",
    "pauses_dataloader = generate_dataloader(test_pauses_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 0\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:44<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.256237\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 1\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:41<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721726\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 2\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:33<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723159\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 3\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:36<00:00, 17.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723159\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 4\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:52<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.620640\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 5\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [01:12<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.719959\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 6\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:52<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.661932\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 7\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:53<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.394465\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 8\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:40<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.262257\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 9\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.296708\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 10\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:44<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.726027\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 11\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:41<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.713209\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 12\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:45<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.716414\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 13\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:33<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.726686\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 14\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.718528\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 15\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.273829\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 16\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723159\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 17\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721726\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 18\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.521427\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 19\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.266647\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 20\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721391\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 21\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.298617\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 22\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.726686\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 23\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.269033\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 24\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.724925\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 25\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.726360\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 26\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721391\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 27\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.722822\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 28\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723490\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 29\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.633026\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 30\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721726\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 31\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.717841\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 32\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.708581\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 33\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.558821\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 34\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.725250\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 35\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723159\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 36\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.716414\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 37\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.648700\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 38\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.726686\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 39\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.359379\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 40\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.711783\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 41\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.716414\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 42\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.724925\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 43\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723159\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 44\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.722126\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 45\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.328196\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 46\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.711067\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 47\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.698041\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 48\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721768\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 49\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.656051\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 50\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.724593\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 51\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:29<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.723159\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 52\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:33<00:00, 19.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.712770\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 53\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:32<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.721391\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 54\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:32<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.716414\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 55\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.713558\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 56\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.714986\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 57\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:31<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.709277\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 58\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:32<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.267445\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 59\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:30<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.726686\n",
      "******************************\n",
      "******************************\n",
      "Loading models...\n",
      "Models trained to epoch: 60\n",
      "Predicting HOLD/SHIFT labels at pauses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 640/640 [00:32<00:00, 19.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 scores...\n",
      "f-score: 0.300467\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pr/edict the F1 scores using both the models on the pause dataset \n",
    "# TODO: Add mechanism to save the pause predictions here for every model instead of later. \n",
    "f1_scores = {}\n",
    "predicted_df_list = [] \n",
    "for epoch, (s0_model_path, s1_model_path) in enumerate(zip(s0_models,s1_models)):\n",
    "    # Load the models \n",
    "    print(\"*\" * 30)\n",
    "    print(\"Loading models...\")\n",
    "    s0_model, s0_info = load_for_inference(s0_model_path)\n",
    "    s1_model, s1_info = load_for_inference(s1_model_path)\n",
    "    assert s0_info['epoch'] == s1_info['epoch'] == epoch \n",
    "    print(\"Models trained to epoch: {}\".format(s0_info['epoch']))\n",
    "    # Predict pauses \n",
    "    y_trues = []\n",
    "    y_hats = [] \n",
    "    speaker_scores_batches = [] \n",
    "    predict_df = pd.DataFrame(columns=[\"prevSpeaker\", \"trueNextSpeaker\", \n",
    "                        \"trueHoldShift\", \"s0Score\", \"s1Score\",\n",
    "                        \"predictedNextSpeaker\",\"predictedHoldShift\"]) \n",
    "    print(\"Predicting HOLD/SHIFT labels at pauses...\")\n",
    "    with torch.no_grad():\n",
    "        count = 0 \n",
    "        for x_batch, y_batch in tqdm(next_batch(pauses_dataloader), total=len(pauses_dataloader)):\n",
    "            prev_speaker, hold_shift_label, next_speaker = y_batch \n",
    "            speaker_scores = [] \n",
    "            for sx_model, xs in zip((s0_model.eval(), s1_model.eval()),x_batch):\n",
    "                va_predictions = sx_model(xs)\n",
    "                speaker_score = np.mean(np.asarray(np.squeeze(va_predictions)))\n",
    "                speaker_scores.append(speaker_score)\n",
    "            predicted_next_speaker = np.argmax(speaker_scores)\n",
    "            # NOTE: The prediction should NOT be the next speaker but the predicted \n",
    "            # hold shift label. \n",
    "            predicted_hold_shift = int(not prev_speaker == predicted_next_speaker)\n",
    "            y_hats.append( predicted_hold_shift)\n",
    "            y_trues.append(hold_shift_label.item())\n",
    "            speaker_scores_batches.append(deepcopy(speaker_scores))\n",
    "            predict_df.loc[count] = (\n",
    "                prev_speaker.item(), next_speaker.item(), hold_shift_label.item(), speaker_scores[0], \n",
    "                speaker_scores[1],predicted_next_speaker, predicted_hold_shift)\n",
    "            count += 1 \n",
    "    y_trues = np.asarray(y_trues)\n",
    "    y_hats = np.asarray(y_hats)\n",
    "    predicted_df_list.append(deepcopy(predict_df))\n",
    "    print(\"Calculating f1 scores...\")\n",
    "    eval_score = f1_score(y_trues, y_hats, average=\"weighted\")\n",
    "    print(\"f-score: {:4f}\".format(eval_score))\n",
    "    f1_scores[epoch] = eval_score\n",
    "    print(\"*\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.25623731783803727,\n",
       " 1: 0.7217261904761905,\n",
       " 2: 0.7231594920338924,\n",
       " 3: 0.7231594920338924,\n",
       " 4: 0.6206397762417752,\n",
       " 5: 0.7199590237337062,\n",
       " 6: 0.6619318181818181,\n",
       " 7: 0.394464746976635,\n",
       " 8: 0.2622574318744839,\n",
       " 9: 0.2967078451349974,\n",
       " 10: 0.7260266178507817,\n",
       " 11: 0.7132091496800581,\n",
       " 12: 0.7164135167227389,\n",
       " 13: 0.7266863344892588,\n",
       " 14: 0.718527640941434,\n",
       " 15: 0.27382883618878673,\n",
       " 16: 0.7231594920338924,\n",
       " 17: 0.7217261904761905,\n",
       " 18: 0.5214271524508666,\n",
       " 19: 0.26664721564636384,\n",
       " 20: 0.7213905526673767,\n",
       " 21: 0.29861723293541476,\n",
       " 22: 0.7266863344892588,\n",
       " 23: 0.26903323495050113,\n",
       " 24: 0.724924740010947,\n",
       " 25: 0.7263599603340787,\n",
       " 26: 0.7213905526673767,\n",
       " 27: 0.7228222508081008,\n",
       " 28: 0.7234896834306538,\n",
       " 29: 0.6330261878927619,\n",
       " 30: 0.7217261904761905,\n",
       " 31: 0.7178414469235971,\n",
       " 32: 0.7085809229049571,\n",
       " 33: 0.5588206197491782,\n",
       " 34: 0.7252495403204623,\n",
       " 35: 0.7231594920338924,\n",
       " 36: 0.7164135167227389,\n",
       " 37: 0.6486996465678759,\n",
       " 38: 0.7266863344892588,\n",
       " 39: 0.35937908253341116,\n",
       " 40: 0.7117833303455621,\n",
       " 41: 0.7164135167227389,\n",
       " 42: 0.724924740010947,\n",
       " 43: 0.7231594920338924,\n",
       " 44: 0.7221263762749663,\n",
       " 45: 0.3281960706634974,\n",
       " 46: 0.7110667449909502,\n",
       " 47: 0.6980407053874345,\n",
       " 48: 0.7217676182666869,\n",
       " 49: 0.6560511664899258,\n",
       " 50: 0.7245929601079816,\n",
       " 51: 0.7231594920338924,\n",
       " 52: 0.7127703971157642,\n",
       " 53: 0.7213905526673767,\n",
       " 54: 0.7164135167227389,\n",
       " 55: 0.7135580871333339,\n",
       " 56: 0.714985737786167,\n",
       " 57: 0.7092767289350803,\n",
       " 58: 0.26744491386956726,\n",
       " 59: 0.7266863344892588,\n",
       " 60: 0.300466790743949}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the predictions for debugging\n",
    "pause_predictions_dir = os.path.join(PROCESSED_DATA_DIR,\"pause_predictions\",FEATURE_SET)\n",
    "if os.path.isdir(pause_predictions_dir):\n",
    "    shutil.rmtree(pause_predictions_dir)\n",
    "os.makedirs(pause_predictions_dir)\n",
    "\n",
    "for i, predicted_df in enumerate(predicted_df_list):\n",
    "    path = os.path.join(pause_predictions_dir,\"pause_prediction_epoch_{}.csv\".format(i))\n",
    "    predicted_df.to_csv(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure Exp-4.2-Figure-4-full\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the F-1 Scores \n",
    "\n",
    "plt.plot(list(f1_scores.values()), label=FEATURE_SET)\n",
    "plt.title(\"F1 score for HOLD / SHIFT at Pauses\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Turn Shift F1 Score\")\n",
    "plt.legend()\n",
    "save_fig(REPORTS_DIR,\"Exp-4.2-Figure-4-{}\".format(FEATURE_SET)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('trp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2304c455ac551f7a45279e05896e7a6c75d8c847965e97ef85e853390a9b358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
